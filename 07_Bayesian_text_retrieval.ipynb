{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMXB0z+c9KDkHQCC3Yg2BV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-stats1/blob/main/07_Bayesian_text_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多項分布を使ったテキスト検索\n",
        "* 以下の二つの方法を試す。\n",
        "\n",
        "(1) MAP推定によって推定したパラメータを利用してクエリの確率を求める。\n",
        "\n",
        "(2) 予測分布を利用してクエリの確率を求める。"
      ],
      "metadata": {
        "id": "ECnVmMCn7SqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ランタイムのタイプをGPUにしておく**"
      ],
      "metadata": {
        "id": "YnIQekV2p3bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (0) 準備"
      ],
      "metadata": {
        "id": "VrgoYRSB8LW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インポート"
      ],
      "metadata": {
        "id": "ClYQyl5JXLuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "Gk0bNQ9Mp_vx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセット"
      ],
      "metadata": {
        "id": "bJrqfKPu8Maq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 20 newsgroupデータセットを使う。\n",
        "  * 最も類似しているテキストが同じクラスに属するかで検索性能を評価する。\n",
        "  * （本当は、情報検索の評価専用のデータセットを使う方が良い。）"
      ],
      "metadata": {
        "id": "MJKZIv863h7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus, train_labels = fetch_20newsgroups(subset=\"train\", return_X_y=True)\n",
        "test_corpus, test_labels = fetch_20newsgroups(subset=\"test\", return_X_y=True)\n",
        "print(f\"training size: {len(train_corpus)}\\ntest size: {len(test_corpus)}\")"
      ],
      "metadata": {
        "id": "NzCwCI7IqB9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f35f05f-48e3-4d86-9a91-717acce89b3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training size: 11314\n",
            "test size: 7532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 単語の出現回数を数える"
      ],
      "metadata": {
        "id": "ZJae3CD4rNX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(min_df=10, stop_words=\"english\")\n",
        "X_train = vectorizer.fit_transform(train_corpus).toarray()\n",
        "X_test = vectorizer.transform(test_corpus).toarray()\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(f\"vocabulary size: {len(vocabulary)}\")"
      ],
      "metadata": {
        "id": "pvdC3n8zqFuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf441835-571e-478d-82d0-eb35a05d74b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size: 15291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) MAP推定\n",
        "* 情報検索の世界では「スムージング」と呼ばれる手法に対応する。"
      ],
      "metadata": {
        "id": "AxV7fi2To5Px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クエリの確率\n",
        "* クエリの確率を、各文書について求めた単語確率を使って計算する。\n",
        "  * $c_{\\mathbf{x}_0,w}$: クエリ$\\mathbf{x}_0$における単語$w$の出現頻度\n",
        "  * このとき、検索対象の文書$\\mathbf{x}$の単語確率$\\boldsymbol{\\phi}_{\\mathbf{x}}$を使って、クエリ$\\mathbf{x}_0$の対数確率を表すと、以下のようになる。\n",
        "$$\\begin{align}\n",
        "\\log p(\\mathbf{x}_0 ; \\boldsymbol{\\phi}_{\\mathbf{x}}) = \\sum_w c_{\\mathbf{x}_0,w} \\log \\phi_{\\mathbf{x},w}\n",
        "\\end{align} + const.$$\n",
        "  * 上の式で、$\\mathbf{x}$に依存しない定数は省略して$const.$と書いている。（検索対象のテキストのランキングに関係しないため。）\n",
        "* このように計算されたクエリの確率の降順に、検索対象のテキストをソートする。\n",
        "  * $\\log p(\\mathbf{x}_0 ; \\boldsymbol{\\phi}_{\\mathbf{x}})$が大きい順に、テキスト$\\mathbf{x} \\in \\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_D\\}$を検索結果として表示する。\n"
      ],
      "metadata": {
        "id": "NAAjHXMVo2lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 計算の工夫\n",
        "* $\\log p(\\mathbf{x}_0 ; \\boldsymbol{\\phi}_{\\mathbf{x}})$の計算は、単に、内積の計算をしているだけ。\n",
        "  * クエリの単語の出現回数$\\mathbf{c}_{\\mathbf{x}_0} \\equiv (c_{\\mathbf{x}_0,1}, \\ldots, c_{\\mathbf{x}_0,W})$と・・・\n",
        "  * 検索対象のテキストの単語確率の対数$\\log \\boldsymbol{\\phi}_{\\mathbf{x}} \\equiv (\\log \\phi_{\\mathbf{x},1}, \\ldots, \\log\\phi_{\\mathbf{x},W})$とで・・・\n",
        "  * 内積の計算$\\mathbf{c}_{\\mathbf{x}_0}^\\top \\log \\boldsymbol{\\phi}_{\\mathbf{x}}$をしているだけ。\n",
        "* ということは・・・\n",
        "* たくさんあるクエリについて、検索対象のテキストの各々で推定された単語確率で尤度を求めることは、行列の積として書ける。\n",
        "* 今回は、GPUも使って、高速化する。"
      ],
      "metadata": {
        "id": "hNdRqeDR5Hwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 下のMAP推定では、ディリクレ事前分布のパラメータ$\\beta_w$がすべて$\\beta$に等しいとする。\n",
        "  * つまり、対称なディリクレ分布とする。"
      ],
      "metadata": {
        "id": "JqRxmPGeNRns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\hat{\\phi}_{\\mathbf{x}, w}\n",
        "& = \\frac{c_{\\mathbf{x},w} + \\beta_w - 1}{\\sum_w (c_{\\mathbf{x},w} + \\beta_w - 1)}\n",
        "\\notag \\\\\n",
        "& = \\frac{c_{\\mathbf{x},w} + \\beta - 1}{l_{\\mathbf{x}} + W\\beta - W}\n",
        "\\mbox{ （今回の設定ではこうなる。） }\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "VAlqN4WNNoPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ディリクレ事前分布のパラメータ\n",
        "beta = 0.01 + 1.0\n",
        "\n",
        "X_train = X_train + beta - 1.0\n",
        "X_train_probs = X_train / X_train.sum(axis=1).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "1xKe491o5eje"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クエリの対数尤度を計算するヘルパ関数\n",
        "* PyTorchを使って、GPU上で計算する。"
      ],
      "metadata": {
        "id": "-bxaiIDqq-zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def log_likelihood(x_test, x_train_prob):\n",
        "  return torch.matmul(\n",
        "      torch.tensor(x_test, dtype=torch.float32, device=\"cuda\"),\n",
        "      torch.log(torch.tensor(x_train_prob, dtype=torch.float32, device=\"cuda\")).t()\n",
        "  ).cpu().numpy()"
      ],
      "metadata": {
        "id": "mrTNOUMu17fW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 検索の実行"
      ],
      "metadata": {
        "id": "CXa-pxljtZMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 全てのクエリ（testテキスト）について、その確率を最大にする検索対象のテキスト（trainingテキスト）求める。\n",
        "  * もちろん、実際の情報検索では、一つ一つのクエリに個別に対応する。\n",
        "  * 下では、時間節約のため、まとめて計算している。"
      ],
      "metadata": {
        "id": "93qv9D1-t-Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = log_likelihood(X_test, X_train_probs)"
      ],
      "metadata": {
        "id": "P-8knQc0qzRS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_train_indices = (- scores).argsort(-1)"
      ],
      "metadata": {
        "id": "orGJTgOSpXrI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_ranked_train_docs = sorted_train_indices[:,0].reshape(-1)\n",
        "print(top_ranked_train_docs)"
      ],
      "metadata": {
        "id": "05cXC360pf7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02a6ea7-dc7d-4f7f-9742-480ad4f25204"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9048 4114 7539 ... 2018 2965 7340]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* P@1はprecision at oneの略。\n",
        "  * https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k"
      ],
      "metadata": {
        "id": "EFsEN7nNtAhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"P@1={(test_labels == train_labels[top_ranked_train_docs]).sum()/len(test_labels):.3f}\")"
      ],
      "metadata": {
        "id": "veu4K7svpngo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3491346a-8a92-43d6-c310-96abeef75973"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P@1=0.703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 最上位にランキングされた訓練文書がテスト文書と同じカテゴリになっている割合は、0.7ぐらい。"
      ],
      "metadata": {
        "id": "ilz8m-99vhbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 5\n",
        "precision = (\n",
        "    train_labels[sorted_train_indices[:,:top_k]]\n",
        "    == test_labels.reshape(-1, 1)\n",
        "    ).sum() / (len(test_labels) * top_k)\n",
        "print(f\"P@{top_k}={precision:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEtgIIMdBVaG",
        "outputId": "bbb13022-deff-4140-dce4-7e35a8741c50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P@5=0.601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* あとは`beta`をチューニングして性能を出す。"
      ],
      "metadata": {
        "id": "4lsjrEqYb4sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) ベイズ推測\n",
        "* 予測分布（＝ディリクレ多項分布）を利用してクエリの予測確率を求める。"
      ],
      "metadata": {
        "id": "oFeO_566K-my"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クエリの予測確率\n",
        "* 下の式の$\\mathbf{x}$のところに検索対象のテキストを代入して、クエリ$\\mathbf{x}_0$の予測確率を計算する。\n",
        "  * $c_{\\mathbf{x},w}$は検索対象のテキストにおける単語$w$の出現回数を表す。\n",
        "  * $c_{\\mathbf{x}_0,w}$はクエリにおける単語$w$の出現回数を表す。"
      ],
      "metadata": {
        "id": "ZbF0UVJmcOeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "p(\\mathbf{x}_0|\\mathbf{x};\\mathbf{\\beta})\n",
        "= \\frac{n_0! \\Gamma(\\sum_{w=1}^W (c_{\\mathbf{x},w} + \\beta_w))}{\\Gamma( \\sum_{w=1}^W (c_{\\mathbf{x},w} + c_{\\mathbf{x}_0,w} + \\beta_w) )}\n",
        "\\prod_{w=1}^W\n",
        "\\frac{\\Gamma(c_{\\mathbf{x},w} + c_{\\mathbf{x}_0,w}+\\beta_w)}{c_{\\mathbf{x}_0,w}!\\Gamma(c_{\\mathbf{x},w} + \\beta_w)}\n",
        "$$"
      ],
      "metadata": {
        "id": "HxvdGLKALA80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 上の式で、$\\mathbf{x}_0$がクエリに相当する。\n",
        "  * よって、$\\mathbf{x}_0$だけに依存する項は、検索対象のテキストのランク付けには無関係。\n",
        "* ディリクレ事前分布は対称ディリクレ分布だと仮定する。\n",
        "  * つまり、すべての$w$について$\\beta_w = \\beta$と、同じ値$\\beta$を取ると仮定する。"
      ],
      "metadata": {
        "id": "ISVwq0j5LkM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 以上を踏まえて、テキスト$\\mathbf{x}$を使って算出されるクエリ$\\mathbf{x}_0$の対数予測確率を書き下す。\n",
        "  * テキスト$\\mathbf{x}$の長さを$l_{\\mathbf{x}}$と書くことにする。"
      ],
      "metadata": {
        "id": "0kq2wNifMnQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\ln p(\\mathbf{x}_0|\\mathbf{x}_i;\\mathbf{\\beta})\n",
        "= \\ln \\Gamma(l_\\mathbf{x} + W\\beta) - \\ln \\Gamma( l_\\mathbf{x} + l_{\\mathbf{x}_0} + W \\beta )\n",
        "+ \\sum_{w=1}^W \\big(\n",
        "\\ln \\Gamma(c_{\\mathbf{x}, w}+c_{\\mathbf{x}_0,w}+\\beta_w) - \\ln \\Gamma(c_{\\mathbf{x}, w} + \\beta_w) \\big) + const.\n",
        "$$"
      ],
      "metadata": {
        "id": "YRwSFh9nL2BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X_train_cuda = torch.tensor(X_train, dtype=torch.float32, device=\"cuda\")\n",
        "train_len = X_train_cuda.sum(-1)\n",
        "X_test_cuda = torch.tensor(X_test, dtype=torch.float32, device=\"cuda\")\n",
        "test_len = X_test_cuda.sum(-1)"
      ],
      "metadata": {
        "id": "xn5GdTAcP2kU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 定数の設定"
      ],
      "metadata": {
        "id": "zFdain5oT9ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.01 #対称ディリクレ事前分布のパラメータ\n",
        "vocab_size = X_train.shape[-1]"
      ],
      "metadata": {
        "id": "kk3tsO1TQq_a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $\\ln \\Gamma(l_\\mathbf{x} + W\\beta)$を計算する。"
      ],
      "metadata": {
        "id": "rV27by0fTRxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lgamma_all = torch.lgamma(train_len + X_train.shape[-1] * beta)"
      ],
      "metadata": {
        "id": "T7zZ-nNsTTl4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $l_\\mathbf{x} + l_{\\mathbf{x}_0}$をブロードキャストで計算する。\n",
        "  * 検索対象のテキストとテスト用テキスト（クエリとして使用）の\n",
        "  * すべての組み合わせについて\n",
        "  * 二つのテキストの長さの和を求める。"
      ],
      "metadata": {
        "id": "ftmR-9gwTjez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_train_len = train_len + test_len.unsqueeze(1)"
      ],
      "metadata": {
        "id": "nFi-w3AZR38P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $\\ln \\Gamma( l_\\mathbf{x} + l_{\\mathbf{x}_0} + W \\beta )$を計算する。"
      ],
      "metadata": {
        "id": "vuqv4xQjTLtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgamma_len = torch.lgamma(test_train_len + vocab_size * beta)"
      ],
      "metadata": {
        "id": "-pK-6YiaTAa-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgamma_len.shape"
      ],
      "metadata": {
        "id": "gT09zsGtWZIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3321543-3c04-4a4b-a112-163da92a9cce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7532, 11314])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $\\ln \\Gamma(c_{\\mathbf{x}, w} + \\beta_w)$を計算する。"
      ],
      "metadata": {
        "id": "EQUlEQeDTdSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lgamma_word = torch.lgamma(X_train_cuda + beta)"
      ],
      "metadata": {
        "id": "zvJbkxw1TfQs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $\\ln \\Gamma(l_\\mathbf{x} + W\\beta)\n",
        "- \\sum_{w=1}^W\n",
        "\\ln \\Gamma(c_{\\mathbf{x}, w} + \\beta_w)$を計算する。"
      ],
      "metadata": {
        "id": "xRa_GgVdUhxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lgamma = train_lgamma_all - train_lgamma_word.sum(-1)"
      ],
      "metadata": {
        "id": "jN2Ev2dnQc3z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lgamma.shape"
      ],
      "metadata": {
        "id": "UN4EK2_1Q7xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec6683a-7cee-4921-ddff-f485ac4a476d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11314])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $c_{\\mathbf{x}, w}+c_{\\mathbf{x}_0,w}$をブロードキャストで計算する。"
      ],
      "metadata": {
        "id": "h6aqyXo7VEi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_sum = X_train_cuda + X_test_cuda.unsqueeze(1)"
      ],
      "metadata": {
        "id": "j7QqfFWDVEPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* メモリが足りないというエラーが出るので、ミニバッチ方式で計算することにする。\n",
        "  * もちろん、実際の情報検索では、一つ一つのクエリに個別に対応する。"
      ],
      "metadata": {
        "id": "TJj0t0noVapH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def log_pred_prob(idx1, idx2):\n",
        "  X_sum = X_train_cuda + X_test_cuda[idx1:idx2].unsqueeze(1) + beta\n",
        "  log_prob = train_lgamma.reshape(1, -1) - lgamma_len[idx1:idx2]\n",
        "  log_prob = log_prob + torch.lgamma(X_sum).sum(-1)\n",
        "  return log_prob"
      ],
      "metadata": {
        "id": "WmlsAx41OdNF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "n_correct_answers = 0\n",
        "for idx in tqdm(range(0, X_test.shape[0], BATCH_SIZE)):\n",
        "  sorted_train_indices = (- log_pred_prob(idx, idx+BATCH_SIZE)).argsort(-1)\n",
        "  top_ranked_train_docs = sorted_train_indices[:,0].reshape(-1)\n",
        "  n_correct_answers += (\n",
        "      test_labels[idx:idx+BATCH_SIZE] == train_labels[top_ranked_train_docs.cpu()]\n",
        "      ).sum()"
      ],
      "metadata": {
        "id": "AV5jYI0jWokA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b40d12-e5b2-48db-e852-4d803d8e17be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1883/1883 [02:39<00:00, 11.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"P@1={n_correct_answers / len(test_labels):.3f}\")"
      ],
      "metadata": {
        "id": "qatQw9bPbjT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "top_k = 5\n",
        "\n",
        "n_correct_answers = 0\n",
        "for idx in tqdm(range(0, X_test.shape[0], BATCH_SIZE)):\n",
        "  sorted_train_indices = (- log_pred_prob(idx, idx+BATCH_SIZE)).argsort(-1)\n",
        "  n_correct_answers += (\n",
        "      train_labels[sorted_train_indices[:,:top_k].cpu()]\n",
        "      == test_labels[idx:idx+BATCH_SIZE].reshape(-1, 1)\n",
        "      ).sum()\n",
        "print(f\"P@{top_k}={n_correct_answers / (len(test_labels) * top_k):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRSAD60xDBrw",
        "outputId": "3d634d66-ca56-4754-8b79-bc918075023f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1883/1883 [02:37<00:00, 11.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P@5=0.580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* あとは`beta`をチューニングする。"
      ],
      "metadata": {
        "id": "ze90ZcqjbvKj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XBAfmWobtN8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}